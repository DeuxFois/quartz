<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Data-sciences on</title><link>https://deuxfois.github.io/quartz/data-science/</link><description>Recent content in Data-sciences on</description><generator>Hugo -- gohugo.io</generator><language>fr-fr</language><atom:link href="https://deuxfois.github.io/quartz/data-science/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://deuxfois.github.io/quartz/data-science/data-analysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/data-analysis/</guid><description>[[data-science/data-analysis/introduction]]
Annalyze result bias-variance
model-validation
Annalyze data [[data-science/data-analysis/covariances-correlation]]
[[data-science/data-analysis/acp]]</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/data-analysis/acp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/data-analysis/acp/</guid><description>Objectif Condenser l’information de la matrice des donn´ees afin d’en retirer les relations caractéristiques (ressemblances entre observations et liaisons entre variables) tout en limitant la perte d’information.</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/data-analysis/bias-variance/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/data-analysis/bias-variance/</guid><description>Introduction consider a function $\mathcal{f} : \mathcal{X} \rightarrow \mathcal{Y}$ produced by some learning algorithm. The prediction of this function can be evaluated through a loss $$ \ell: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$$ such that $\mathcal{l}(y, f(x)) \geq 0$ measures how close the prediction $\mathcal{f}(x)$ from $y$ is</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/data-analysis/covariances-correlation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/data-analysis/covariances-correlation/</guid><description>Covariance et Corrélations Matrice des variances-covariances $$ \begin{aligned} S=\operatorname{Var}(\mathbf{X}) &amp;amp;=\left[\begin{array}{ccccc} \operatorname{Var}\left(X_{1}\right) &amp;amp; \operatorname{Cov}\left(X_{1}, X_{2}\right) &amp;amp; \cdots &amp;amp; \operatorname{Cov}\left(X_{1}, X_{d}\right) \ \operatorname{Cov}\left(X_{2}, X_{1}\right) &amp;amp; \ddots &amp;amp; \cdots &amp;amp; \operatorname{Cov}\left(X_{2}, X_{d}\right) \ \vdots &amp;amp; &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \ \operatorname{Cov}\left(X_{d}, X_{1}\right) &amp;amp; \operatorname{Cov}\left(X_{d}, X_{2}\right) &amp;amp; \cdots &amp;amp; \operatorname{Var}\left(X_{d}\right) \end{array}\right] \ &amp;amp;=\left[\begin{array}{cccc} s_{X_{1}}^{2} &amp;amp; s_{X_{1}, X_{2}}^{2} &amp;amp; \cdots &amp;amp; s_{X_{1}, X_{d}}^{2} \ s_{X_{2}, X_{1}}^{2} &amp;amp; \ddots &amp;amp; \cdots &amp;amp; s_{X_{2}, X_{d}}^{2} \ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \ s_{X_{d}, X_{1}}^{2} &amp;amp; s_{X_{d}, X_{2}}^{2} &amp;amp; \cdots &amp;amp; s_{X_{d}}^{2} \end{array}\right] \end{aligned} $$ avec $s_{j, j^{\prime}}^{2}$ la covariance entre les variables $X_{j}$ et $X_{j^{\prime}}$, tel que $$ s_{X_{j}, X_{j^{\prime}}}^{2}=\sum_{i=1}^{m} p_{i}\left(x_{i j}-\mu_{j}\right)\left(x_{i j^{\prime}}-\mu_{j^{\prime}}\right) $$ La covariance mesure la liaison linéaire qui peut exister entre un couple de variables quantitatives.</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/data-analysis/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/data-analysis/introduction/</guid><description>Comment décrire les données ? Approche 1 effectuer une analyse descriptive multidimensionelle ⊖ trop longue et souvent trop complexe
Approche 2 : utiliser des méthodes d’analyse des données ex: les méthodes factorielles comme l’Analyse en Composantes Principales (ACP) • Synthèse : réduire la dimension du problème tout en restituant le maximum d’information • Descriptif et exploratoire : visualisation des données (production de graphiques simples)</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/data-analysis/model-validation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/data-analysis/model-validation/</guid><description>Holdout-out Validation Method It is considered one of the easiest model validation techniques helping you to find how your model gives conclusions on the holdout set.</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/deep-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/deep-learning/</guid><description>the Perceptron the root of deep learning. It was initially intended as an image recognition machine. It gets its name from performing the human-like function of perception, seeing and recognizing images.</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/deep-learning/introduction-to-pytorch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/deep-learning/introduction-to-pytorch/</guid><description>1 2 3 # Importing both packages import torch.nn as nn import torch.optim as optim nn.Module 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class MySimpleMLP(nn.</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/deep-learning/LDA-and-Sigmoid/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/deep-learning/LDA-and-Sigmoid/</guid><description>Consider training data $(\mathbf{x},y)\sim P(X,Y)$ with
$\mathbf{x}\in\mathbb{R}^{p},$ $y \in {0,1}.$ $P({\bf x}|y)=\frac{1}{\sqrt{(2\pi)^{p}|\Sigma|}}\exp\left(-\frac{1}{2}({\bf x}-\mu_{y})^{T}\Sigma^{-1}({\bf x}-\mu_{y})\right)$ Using the Baye&amp;rsquo;s rule, we have $$ \begin{aligned} P(Y=1 \mid \mathbf{x}) &amp;amp;=\frac{P(\mathbf{x} \mid Y=1) P(Y=1)}{P(\mathbf{x})} \ &amp;amp;=\frac{P(\mathbf{x} \mid Y=1) P(Y=1)}{P(\mathbf{x} \mid Y=0) P(Y=0)+P(\mathbf{x} \mid Y=1) P(Y=1)} \ &amp;amp;=\frac{1}{1+\frac{P(\mathbf{x} \mid Y=0) P(Y=0)}{P(\mathbf{x} \mid Y=1) P(Y=1)}} .</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/deep-learning/multi-layer-perceptron/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/deep-learning/multi-layer-perceptron/</guid><description/></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/deep-learning/perceptron/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/deep-learning/perceptron/</guid><description>Introduction the perceptron is a mathematical model. Originally, he was inspired by the human brain and was constitued by, what we might call, an artificial neuron.</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/deep-learning/vanishing-gradient/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/deep-learning/vanishing-gradient/</guid><description>The problem: As more layers using certain activation functions are added to neural networks, the gradients of the loss function approaches zero, making the network hard to train.</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/generative-discriminative-2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/generative-discriminative-2/</guid><description>[Note] Variations on supervised and unsupervised Model ? Semi-Supervised Learning: we have a bunch of pairs (x1,y1), (x2,y2), &amp;hellip;(x_i,y_i), and then we are additionally given more x values such as x_i+1, x_i+2,.</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/machine-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/machine-learning/</guid><description>supervised-learning is defined by its use of labeled datasets. These datasets are designed to train or “supervise” algorithms into classifying data or predicting outcomes accurately.</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/machine-learning/generative-vs-discriminative/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/machine-learning/generative-vs-discriminative/</guid><description>Generative vs Discriminative model Introduction A generative model learns the joint probability distribution $p(x,y)$
A discriminative model learns the conditional probability distribution $p(y|x)$ - which you should read as &amp;ldquo;the probability of $y$ given $x$</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/machine-learning/supervised-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/machine-learning/supervised-learning/</guid><description>Apprentissage supervisé On associe à chaque observation $\mathbf{x}_{i}$ une valeur à prédire $y_{i} \in \mathcal{Y}$. Tout comme pour les observation les valeurs à prédire (label) peuvent être concaténées en un vecteur $\mathbf{y} \in \mathcal{Y}^{n}$ L&amp;rsquo;espace des valeurs à prédire $\mathcal{Y}$ sera : $\mathcal{Y}=\lbrace{-1,1}\rbrace$ pour la classification binaire ou $\mathcal{Y}={1, \ldots, m}$ pour la classification multi-classes ( $m$ classes).</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/machine-learning/supervised-learning/batch-vs-stohastic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/machine-learning/supervised-learning/batch-vs-stohastic/</guid><description>Batch vs Stochastic Batch Gradient Descent BGD is a variation of the gradient descent algorithm that calculates the error for each eg in the training datasets, but only updates the model after all training examples have been evaluated.</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/machine-learning/supervised-learning/bayes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/machine-learning/supervised-learning/bayes/</guid><description>⚠️ this article presume you know about the normal law and multivariate normal law
Bayes Law $$ \underbrace{\mathbb{P}(Y \mid X)}_{\text {Posterior probability }}=\frac{\overbrace{\mathbb{P}(Y)}^{\text {Prior probability}} \cdot \overbrace{\mathbb{P}(X \mid Y)}^{\text {Likelihood}}}{\mathbb{P}(X)} $$ the denominator is given by $p(x)={p(x|y=1)}p(y=1)+{p(x|y=0)}p(y=0)$</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/machine-learning/supervised-learning/bayes/LDA-vs-LogisticRegression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/machine-learning/supervised-learning/bayes/LDA-vs-LogisticRegression/</guid><description/></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/machine-learning/supervised-learning/knn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/machine-learning/supervised-learning/knn/</guid><description>The K-Nearest Neighbours (KNN) algorithm is one of the simplest supervised machine learning algorithms that is used to solve both classification and regression problems.</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/machine-learning/supervised-learning/linear-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/machine-learning/supervised-learning/linear-regression/</guid><description>Linear Regression Pros Quick to compute and can be updated easily with new data Relatively easy to understand and explain Regularization techniques can be used to prevent overfitting</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/machine-learning/supervised-learning/logistic-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/machine-learning/supervised-learning/logistic-regression/</guid><description>⚠️ this article presume you know about the linear-regression
Logistic Regression uses a sigmoid function, also known as the &amp;rsquo;logistic function&amp;rsquo;.</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/machine-learning/unsupervised-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/machine-learning/unsupervised-learning/</guid><description>Apprentissage non-supervisé $\mathbf{x} \in \mathbb{R}^{d}$ est une observation de $d$ variables réelles. L&amp;rsquo;ensemble d&amp;rsquo;apprentissage est définit par les observations $\lbrace{x}_{i}\rbrace_{i=1}^{m}$ où $n$ est le nombre d&amp;rsquo;exemples d&amp;rsquo;apprentissages (de points).</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/machine-learning/unsupervised-learning/kmeans/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/machine-learning/unsupervised-learning/kmeans/</guid><description>Kmeans centroïde $\boldsymbol{\mu}=\frac{1}{m} \sum_{i}^{m} \mathbf{x}_{i}$ inertie $\mathcal{I}{T}=\sum{\mathbf{x}{i}}\left|\mathbf{x}{i}-\boldsymbol{\mu}\right|^{2}$ ![[Pasted image 20220406110758.png]]
Inertie L&amp;rsquo;inertie $\mathcal{I}{T}$ d&amp;rsquo;un nuage des points est représentée par la distance au carré des points à leur centroïde $$\sum{\mathbf{x}{i}}\left|\mathbf{x}{i}-\boldsymbol{\mu}\right|^{2}=\sum_{c=1} m_{c}\left|\boldsymbol{\mu}{c}-\boldsymbol{\mu}\right|^{2} \quad+\sum{c=1} \sum_{\mathbf{x}{i} \mid \hat{y}{i}=c}\left|\mathbf{x}{i}-\boldsymbol{\mu}{c}\right|^{2} $$</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/statistic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/statistic/</guid><description>Probability Probability is the likelihood of an event. To define an event, we use random variables in a sample space, who can be a continuous ($x \in \mathbb{R}$) or discrete set ($x \in \mathbb{N }$) .</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/statistic/Arrangement/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/statistic/Arrangement/</guid><description>Arrangement sans répétition Un arrangement sans répétition est noté:
$$A_n^{k}$$
Un arrangement avec répétions c&amp;rsquo;est prendre k billes parmi n dans un sac de billes, en tenant compte de l&amp;rsquo;ordre dans lequel on sort chacune d&amp;rsquo;elles.</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/statistic/binomial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/statistic/binomial/</guid><description>Soit une urne contenant des boules rouges et des boules blanches pour un total de n boules, on s’intéresse à la variable aléatoire K : nombre de boules rouges tirées parmi les n boules .</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/statistic/conditionelles-bayes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/statistic/conditionelles-bayes/</guid><description>Probabilité conditionnelle: Si l’événement A se réalise, les événements possibles deviennent en effet l’ensemble des parties de A, et non plus l’ensemble des parties de Ω.</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/statistic/Covariance-correlation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/statistic/Covariance-correlation/</guid><description>Covariance et corrélation Covariance La covariance est une mesure de la façon dont deux variables varient ensemble. Par exemple la taille et le poids des girafes ont des covariances positives car quand l&amp;rsquo;une est grande, l&amp;rsquo;autre a tendance à l&amp;rsquo;être aussi.</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/statistic/gaussian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/statistic/gaussian/</guid><description>Normal law In Statistics and Probability theory, normal laws are among the most used laws to model phenomena. She is related to many mathematical objects including Brownian motion, Gaussian white noise or other probability laws.</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/statistic/independance/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/statistic/independance/</guid><description>Distributions jointes et indépendance Si on a plusieurs variables, on a une loi de probabilité à plusieurs variables (joint probability mass function) qu&amp;rsquo;on note p(xi, yi) pour le cas discret, et f(xi,yi) pour le cas continu.</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/statistic/intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/statistic/intro/</guid><description>Théorème des probabilités composées La définition même des probabilités conditionnelles permet d&amp;rsquo;écrire que : $$ p(A \cap B)=p(A) \times p(B \mid A) $$ et aussi que: $$ p(A \cap B)=p(B) \times p(A \mid B) $$ C&amp;rsquo;est le théorème des probabilités composées, que l&amp;rsquo;on peut énoncer ainsi : si un événement résulte du concours de deux événements, sa probabilité est égale à celle de l&amp;rsquo;un d&amp;rsquo;eux multipliée par la probabilité conditionnelle de l&amp;rsquo;autre sachant que le premier est réalisé.</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/statistic/variable-aleatoire/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/statistic/variable-aleatoire/</guid><description>Variable aléatoire Discrète La fonction de masse (probability mass function = pmf) décrit la probabilité d&amp;rsquo;obtenir chacune des issues. On la note p(x).</description></item></channel></rss>