<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>generative on</title><link>https://deuxfois.github.io/quartz/tags/generative/</link><description>Recent content in generative on</description><generator>Hugo -- gohugo.io</generator><language>fr-fr</language><atom:link href="https://deuxfois.github.io/quartz/tags/generative/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://deuxfois.github.io/quartz/data-science/machine-learning/supervised-learning/bayes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/machine-learning/supervised-learning/bayes/</guid><description>⚠️ this article presume you know about the normal law and multivariate normal law
Bayes Law $$ \underbrace{\mathbb{P}(Y \mid X)}_{\text {Posterior probability }}=\frac{\overbrace{\mathbb{P}(Y)}^{\text {Prior probability}} \cdot \overbrace{\mathbb{P}(X \mid Y)}^{\text {Likelihood}}}{\mathbb{P}(X)} $$ the denominator is given by $p(x) = {p(x|y=1)}p(y=1)+{p(x|y=0)}p(y=0)$</description></item></channel></rss>