<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>discrete-value on</title><link>https://deuxfois.github.io/quartz/tags/discrete-value/</link><description>Recent content in discrete-value on</description><generator>Hugo -- gohugo.io</generator><language>fr-fr</language><atom:link href="https://deuxfois.github.io/quartz/tags/discrete-value/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://deuxfois.github.io/quartz/data-science/machine-learning/supervised-learning/bayes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/machine-learning/supervised-learning/bayes/</guid><description>⚠️ this article presume you know about the normal law and multivariate normal law
Bayes Law $$ \underbrace{\mathbb{P}(Y \mid X)}_{\text {Posterior probability }}=\frac{\overbrace{\mathbb{P}(Y)}^{\text {Prior probability}} \cdot \overbrace{\mathbb{P}(X \mid Y)}^{\text {Likelihood}}}{\mathbb{P}(X)} $$ the denominator is given by $p(x)={p(x|y=1)}p(y=1)+{p(x|y=0)}p(y=0)$</description></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/machine-learning/supervised-learning/knn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/machine-learning/supervised-learning/knn/</guid><description/></item><item><title/><link>https://deuxfois.github.io/quartz/data-science/machine-learning/supervised-learning/logistic-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deuxfois.github.io/quartz/data-science/machine-learning/supervised-learning/logistic-regression/</guid><description>⚠️ this article presume you know about the linear-regression
Logistic Regression uses a sigmoid function, also known as the &amp;rsquo;logistic function&amp;rsquo;.</description></item></channel></rss>