<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="[Note] Variations on supervised and unsupervised Model ?  Semi-Supervised Learning:  we have a bunch of pairs (x1,y1), (x2,y2), &mldr;(x_i,y_i), and then we are additionally given more x values such as x_i+1, x_i+2,."><title>ü™¥ Quartz 3.2</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://quartz.jzhao.xyz//icon.png><link href=https://quartz.jzhao.xyz/styles.b54f56dc7e7fdf6be9a4427dc6b0509b.min.css rel=stylesheet><link href=https://quartz.jzhao.xyz/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://quartz.jzhao.xyz/js/darkmode.63d6a3e095d0bd2b935b62adec9dc11b.min.js></script>
<script src=https://quartz.jzhao.xyz/js/util.39f53d45cb9520bdaf946bd063598b19.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://quartz.jzhao.xyz/js/popover.37b1455b8f0603154072b9467132c659.min.js></script>
<script src=https://quartz.jzhao.xyz/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://quartz.jzhao.xyz/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://quartz.jzhao.xyz/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://quartz.jzhao.xyz/",fetchData=Promise.all([fetch("https://quartz.jzhao.xyz/indices/linkIndex.138e8e684375e828e1fe07635a41b9e8.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://quartz.jzhao.xyz/indices/contentIndex.10367ca03749dc74c72b80b9868d2b00.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const n=new URL(BASE_URL),s=n.pathname,o=window.location.pathname,i=s==o;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts();const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=i&&!1;drawGraph("https://quartz.jzhao.xyz",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2}),initPopover("https://quartz.jzhao.xyz",!0,!0)},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/quartz.jzhao.xyz\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-XYFD95KB4J"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XYFD95KB4J",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://quartz.jzhao.xyz/js/full-text-search.0ec5fb908204b9e13989f2c640c0870b.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://quartz.jzhao.xyz/>ü™¥ Quartz 3.2</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><p class=meta>Last updated
Aug 2, 2022
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/MLIA/Generative%20vs%20Discriminative++.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><ol><li><a href=#note-variations-on-supervised-and-unsupervised-model->[Note] Variations on supervised and unsupervised Model ?</a></li><li><a href=#note-pgm->[Note] PGM ?</a></li></ol></li></ol><ol><li><a href=#a-generative-analysis>A> Generative Analysis</a></li><li><a href=#1-linear-discriminant-analysis>1. Linear Discriminant Analysis</a></li></ol><ol><li><a href=#2-latent-dirichlet-allocation>2. Latent Dirichlet Allocation</a></li><li><a href=#3-autoencoder>3. AutoEncoder</a><ol><li><a href=#variational-inference--neural-network--scalable-vi>Variational Inference + Neural Network = Scalable VI</a></li></ol></li><li><a href=#-background-knowledge>> Background Knowledge:</a></li><li><a href=#now-lets-find-the-lower-bound-to-estimate-the-mle-value>Now, Let&rsquo;s find the Lower Bound to estimate the <code>MLE value</code>!</a></li><li><a href=#let-me-introduce-qt-as-the-variational-distribution-of-the-alpha-coefficient-mixing-coefficient-probability-of-the-hidden-membership-t-c-here-qt-are-not-fingers-any-more-like-in-the-typical-variational-inference--jansens-lower-boundsss-are-fingers-any-distribution-can-be-estimated-by-such-a-bunch-of-lower-bounds>Let me introduce <code>q(t)</code> as the variational distribution of the <code>alpha coefficient</code> (mixing coefficient: probability of the hidden membership <code>t</code>= c). Here, <code>q(t)</code> are not fingers any more like in the typical Variational Inference. Jansen&rsquo;s &ldquo;lower boundsss&rdquo; are fingers. Any distribution can be estimated by such a bunch of &ldquo;lower bounds&rdquo;!!!</a></li><li><a href=#is-it-just-coincidence-that-jansens-lower-bound-looks-like-kl-divergence>Is it just coincidence that <code>Jansen's lower bound</code> looks like <code>KL-Divergence</code>?</a></li><li><a href=#now-we-just-found-the-first-jansens-bound-as-a-finger-how-many-more-fingers-to-go>Now we just found the first Jansen&rsquo;s bound as a finger. How many more fingers to go?</a></li></ol><ol><li><a href=#lets-model-the-image-px--yes-its-about-damn-large-sample-size-with-high-dimensionalityin-the-context-of-unsupervised-learning>Let&rsquo;s model the image <code>P(x)</code> ! Yes, it&rsquo;s about damn large sample size with high dimensionality..in the context of Unsupervised Learning.</a><ol><li><a href=#3-our-pick-is-pdf-this-is-very-important-lets-find-the-density-model-of-our-data-predictive-distribution>[3. Our pick is pdf] This is very important. Let&rsquo;s find the density model of our data (predictive distribution)!</a></li></ol></li><li><a href=#you-know-what-we-are-start-off-with-decoder->You know what? we are start off with Decoder ?</a></li><li><a href=#overview-get-some-variational-distribution-qt-or-lbt->Overview: get some variational distribution <code>q(t)</code> or <code>LB(t)</code> ?</a></li><li><a href=#t-and-w-is-the-key><code>t</code> and <code>w</code> is the key!</a></li><li><a href=#we-found-qt--ptx--nm-s2-which-is-an-unique-mixing-coefficient-functioninterestingly-we-forget-about-the-mixing-coefficient-and-simply-do-montecarlo-sampling-from-this-distributionqt-to-get-random-data-pt-t>We found <code>q(t)</code> = <code>P(t|x)</code> = <code>N(m, s^2)</code> which is an unique mixing coefficient function&mldr;Interestingly, we forget about the mixing coefficient and simply do MonteCarlo Sampling from this distribution<code>q(t)</code> to get random data pt <code>t</code>.</a></li><li><a href=#next-how-to-define-the-cnns-weighting-mechanism-for-œÜ-and-w---keep-maximizing-jensens-lower-bound-via-each-gradient-calculation-in-cnn>Next, how to define the CNN&rsquo;s weighting mechanism for <code>Œ¶</code> and <code>w</code> ? : Keep maximizing <code>Jensen's Lower bound</code> via each gradient calculation in CNN!</a><ol><li><a href=#general-learning-with-latent-priors>General Learning with latent priors</a></li><li><a href=#variational-dropout-and-scalable-bnn>Variational Dropout and Scalable BNN</a></li></ol></li></ol></nav></details></aside><a href=#note-variations-on-supervised-and-unsupervised-model-><h3 id=note-variations-on-supervised-and-unsupervised-model-><span class=hanchor arialabel=Anchor># </span>[Note] Variations on supervised and unsupervised Model ?</h3></a><ul><li>Semi-Supervised Learning:<ul><li>we have a bunch of pairs (<strong>x</strong>1,<strong>y</strong>1), (<strong>x</strong>2,<strong>y</strong>2), &mldr;(<strong>x</strong>_i,<strong>y</strong>_i), and then we are additionally given more x values such as x_i+1, x_i+2,..up to x_n. Our task is to predict <code>y_i+1</code>, <code>y_i+2</code>,..up to <code>y_n</code>.</li></ul></li><li>Reinforcement Learning:<ul><li>Investigate the &ldquo;reward/loss&rdquo;(long/short term payoff) associated with a certain action or state..</li></ul></li></ul><a href=#note-pgm-><h3 id=note-pgm-><span class=hanchor arialabel=Anchor># </span>[Note] PGM ?</h3></a><p>If you are keen on studying generative models and delving deeper into them, I would say concepts and thorough knowledge on Probabilistic graphical models is essential. If your focus is on Discriminative models or planning to use deep learning as a blackbox then you can get away without PGM and its probably not very essential. But if you are planning for a research either in implicit or explicit generative models or especially deep generative models, then I strongly recommend PGM as a course. Its a valuable tool for sure</p><a href=#generative-vs-discriminative-model><h1 id=generative-vs-discriminative-model><span class=hanchor arialabel=Anchor># </span>Generative VS Discriminative Model</h1></a><p>Machine Learning models can be typically divided into two types. Discriminative and Generative models. Discriminative models deal with classification or categorization of the data, examples include SVMs, linear/logistic regression, CRFs etc. Generative approaches model how the data is generated and various other tasks can be further addressed with this generative information, examples include HMMs, GMM, Boltzman machines. Deep learning models such as DBMs, Variational AutoEncoders are again generative models.</p><ul><li><p><strong>[A]. Generative algorithm:</strong> learning each structure, and then classifying it using the knowledge you just gained.</p><ul><li>A generative model is a statistical model of the joint probability distribution on <code>P(X,Y)</code> and Classifiers are computed using probablistic models.</li><li>Generative modeling means building a model that can generate new examples that come from the same distribution as the training data (or that can look at an input example and report the likelihood of it being generated by that distribution). This means generative modeling is a kind of unsupervised learning.</li><li>A generative algorithm models uses the data to create a <strong><code>probabilities</code></strong>, and how the data was &ldquo;generated&rdquo;, so you ask it &ldquo;what&rsquo;s the likelihood this or that class generated this instance?&rdquo; and pick the one with the <strong>better probability</strong>.<ul><li>Estimate joint probability ### P(Y, X) = P(Y|X)f(X) = f(X|Y)P(Y)<ul><li>where Y is label(class), <code>f() is pdf</code> and <code>P() is class marginal probability</code>.</li><li><code>f(X|Y)P(Y)</code> : first choose a class, then given the class, we choose(generate) the point X.</li><li>P(Y|X)f(X) : first choose the point X, then given the point, we choose a class. This is discriminative though.</li></ul></li><li>Estimates not only probability of labels but also the features</li><li>Once model is fit, can be used to generate data, but often works worse, particularly when assumptions are violated</li></ul></li><li>Linear Generative Dimensionality Reduction Algorithm<ul><li>LDA, QDA, PCA, Naive Bayes, etc.</li></ul></li><li>Nonlinear Generative Dimensionality Reduction Algorithm<ul><li>AutoEncoder, Variational AutoEncoder, etc.</li></ul></li></ul></li><li><p><strong>[B]. Discriminative algorithm:</strong> determining the difference in the each without learning the structure, and then classifying the data_point.</p><ul><li>A discriminative model is a statistical model of the conditional probability distribution on <code>P(Y|X=x)</code> and Classifiers computed <strong>without using a probability model</strong> are also referred to loosely as &ldquo;discriminative&rdquo;.</li><li>A discriminative algorithm uses the data to create a <strong><code>decision boundary</code></strong>, so you ask it &ldquo;what side of the decision boundary is this instance on?&rdquo; So it doesn&rsquo;t create a model of how the data was generated, it makes a model of what it thinks the boundary between classes looks like.</li></ul></li></ul><p>Since <strong>discriminative</strong> cares <code>P(Y|X)</code> only, while <strong>generative</strong> cares <code>P(X,Y) and P(X)</code> at the same time, in order to predict <strong>P(Y|X)</strong> well, the generative model has <strong><code>less degree of freedom</code></strong> in the model compared to discriminative model. So generative model is more robust, less prone to overfitting while discriminative is the other way around. So <strong>discriminative models</strong> usually tend to do better if you have <code>lots of data</code>; <strong>generative models</strong> may be better if you have some extra <code>unlabeled or missing data</code>(the generative model has its own advantages such as the capability of dealing with missing data).
<img src=https://quartz.jzhao.xyz//Pasted%20image%2020220704120136.png width=auto alt></p><a href=#a-generative-analysis><h2 id=a-generative-analysis><span class=hanchor arialabel=Anchor># </span>A> Generative Analysis</h2></a><hr><a href=#1-linear-discriminant-analysis><h2 id=1-linear-discriminant-analysis><span class=hanchor arialabel=Anchor># </span>1. Linear Discriminant Analysis</h2></a><ul><li>As a <strong>Supervised method</strong>, labels are used to learn the <code>data structure</code> which allows the <strong>classification</strong> of future observations.</li></ul><a href=#pgx><h1 id=pgx><span class=hanchor arialabel=Anchor># </span><code>P(g|x)</code></h1></a><ul><li><p>Predict the membership of the given vector <code>x</code>.</p></li><li><p>We have a dataset containing lots of vector observations(rows) and their labels.</p></li><li><p>What&rsquo;s the probability that the new vector observation <code>x</code> belongs to the Grp <code>g</code>? (p is the dimension of the vector x).</p></li><li><p>This probabilities come from a certain <strong>likelihood distribution of Grp</strong>(with different parametrization)&mldr;in detail,
<img src=https://user-images.githubusercontent.com/31917400/52278515-2156c280-294f-11e9-9bc2-6e40c4563b8f.jpg></p></li><li><p>So let&rsquo;s figure out the Likelihood distribution <code>P(x|g)</code>. This is the distribution of data points in each group. If we know the <strong>distribution of x in each Grp: <code>P(x|g)</code></strong>, we can classify the new p-dimensional data points given in the future&mldr;so done and dusted. What if choosing the Grp_feature distribution <code>P(x|g)</code> as <strong>multivariate Gaussian</strong> ? (Of course, in the multivariate version, <code>¬µ</code> is a mean vector and œÉ is replaced by a covariance matrix <code>Œ£</code>).
<img src=https://user-images.githubusercontent.com/31917400/52270233-3d9b3500-2938-11e9-9585-63ef137328a4.jpg></p></li></ul><a href=#two-functions-to-maximize-pgx><h1 id=two-functions-to-maximize-pgx><span class=hanchor arialabel=Anchor># </span>two functions to maximize <code>P(g|x)</code>.</h1></a><ul><li>Assumption: <strong>all Grp share the equal <code>Œ£</code> matrix</strong>(in QDA, the equal covariance assumption does not hold, thus you cannot drop <code>-0.5log(|Œ£|)</code> term).</li><li>Which &lsquo;g&rsquo; has the highest probability of owning the new p-dimensional datapoint?<ul><li><p>Eventually, Min/Max depends on the unique parameter(<code>¬µ,Œ£</code>) of each Grp.</p></li><li><p>When you plug in x vector, <code>¬µ,Œ£</code> that minimizing <strong>Mahalonobis Distance</strong>, is telling you the membership of the vector <code>x</code>.<br><img src=https://user-images.githubusercontent.com/31917400/52273637-57417a00-2942-11e9-8881-f7279ec947d4.jpg></p></li><li><p>When you plug in x vector, <code>¬µ,Œ£</code> that maximizing <strong>LD-function</strong>, is telling you the membership of the vector <code>x</code>.
<img src=https://user-images.githubusercontent.com/31917400/52273639-59a3d400-2942-11e9-900e-077ceabfb0b9.jpg></p></li></ul></li></ul><blockquote><p>In practice,
<img src=https://user-images.githubusercontent.com/31917400/52275738-4dbb1080-2948-11e9-9768-3da4a0c5c773.jpg></p></blockquote><blockquote><p><strong>Log Odd Ratio</strong> and <code>Linear Decision Boundary</code></p></blockquote><ul><li>What if the Grp membership probability of &lsquo;g1&rsquo;, &lsquo;g2&rsquo; are the same?</li><li>Then we can say that the given vector point is the part of <code>Linear Decision Boundary</code> !!!
<img src=https://user-images.githubusercontent.com/31917400/52283578-a2678700-295a-11e9-98ae-817a9f91afdc.jpg></li></ul><blockquote><p>LDA and Logistic regression</p></blockquote><ul><li>LDA is Generative while LogisticRegression is discriminative.</li><li>LDA operates by maximizing the log-likelihood based on an assumption of normality and homogeneity while Logistic regression makes no assumption about P(X), and estimates the parameters of P(g|x) by maximizing the conditional likelihood.</li><li>logistic regression would presumably be more robust if LDA‚Äôs distributional assumptions (Gaussian?) are violated.</li><li>In principle, LDA should perform poorly when outliers are present, as these usually cause problems when assuming normality.</li><li>In LDA, the log-membership odd between Grps are <strong>linear functions</strong> of the vector data x. This is due to the assumption of <code>Gaussian densities</code> and <code>common covariance matrices</code>.</li><li>In LogisticRegression, the log-membership odd between Grps are <strong>linear functions</strong> of the vector data x as well.
<img src=https://user-images.githubusercontent.com/31917400/52282688-e6f22300-2958-11e9-923a-5be3e22e8de9.jpg></li></ul><a href=#2-latent-dirichlet-allocation><h2 id=2-latent-dirichlet-allocation><span class=hanchor arialabel=Anchor># </span>2. Latent Dirichlet Allocation</h2></a><p>The finite Dirichlet distribution is a distribution over distributions, namely over multinomial distributions. That means if you draw a sample from it, you get a random distribution. A loaded die can be described by a <code>multinomial distribution</code>. A machine that makes biased die with some random error can be described by a <code>Dirichlet distribution</code>. Suppose there are boxes with chocolates, with some portion of dark and sweet chocolates. You pick at random one of the boxes(perhaps some kinds of boxes can be more common than others. Then, you can pick at random one of the chocolates. So you have a distribution (a collection of boxes) of distributions (chocolates in a box).</p><ul><li>Just as the beta distribution is the conjugate prior for a binomial likelihood, the Dirichlet distribution is the conjugate prior for the multinomial likelihood. It can be thought of as a <strong>multivariate beta distribution</strong> for a collection of probabilities (that must sum to 1).</li></ul><p>LDA is a ‚Äúgenerative probabilistic model‚Äù of a collection of <strong>composites made up of parts</strong>.</p><ul><li>The composites are <code>documents</code>.</li><li>The <strong>topics</strong> are Latent Variable.</li><li>The parts are <code>words</code></li><li><code>Document</code> is a distribution over <code>topics</code>.</li><li><code>Topic</code> is a distribution of <code>words</code>.
<img src=https://user-images.githubusercontent.com/31917400/67500637-e7635300-f67a-11e9-93f5-ff72ffa0a04b.jpg></li></ul><p>The probabilistic topic model estimated by LDA consists of two tables (matrices):</p><ul><li><p>1st table: the probability of selecting a particular <code>part(word)</code> when sampling a particular <strong>topic(category)</strong>.</p></li><li><p>2nd table: the probability of selecting a particular <strong>topic(category)</strong> when sampling a particular <code>document</code>.
<img src=https://user-images.githubusercontent.com/31917400/52525957-842abf80-2ca9-11e9-8465-b36a9e1d2d4e.jpg></p></li><li><blockquote><p>In the chart above, every topic is given the same alpha value. Each dot represents some distribution or mixture of the three topics like (1.0, 0.0, 0.0) or (0.4, 0.3, 0.3). Remember that each sample has to add up to one. At low alpha values (less than one), most of the topic distribution samples are in the corners (near the topics). For really low alpha values, it‚Äôs likely you‚Äôll end up sampling (1.0, 0.0, 0.0), (0.0, 1.0, 0.0), or (0.0, 0.0, 1.0). This would mean that a document would only ever have one topic if we were building a three topic probabilistic topic model from scratch.</p></blockquote></li><li><blockquote><p>At alpha equal to one, any space on the surface of the triangle (2-simplex) is fair game (uniformly distributed). You could equally likely end up with a sample favoring only one topic, a sample that gives an even mixture of all the topics, or something in between. For alpha values greater than one, the samples start to congregate to the center. This means that as alpha gets bigger, your samples will more likely be uniform or an even mixture of all the topics.</p></blockquote></li><li><p>WTF? <strong>Simplest Generative Procedure:</strong>
<img src=https://user-images.githubusercontent.com/31917400/67502548-ebdd3b00-f67d-11e9-9ac6-c015878416be.jpg></p><ul><li>Pick your unique set of WORDS.</li><li>Pick how many DOCUMENTS you want.</li><li>Pick how many WORDS you want per each DOCUMENT (sample from a Poisson distribution).</li><li>Pick how many <code>topics</code>(categories or label) you want.</li><li>Pick a number between not zero and positive infinity and call it <strong>alpha</strong>.</li><li>Pick a number between not zero and positive infinity and call it <strong>beta</strong>.</li><li>Build the <code>**WORDS** VS **topics** table</code>.<ul><li>For each column, draw a sample(spin the wheel) from a Dirichlet distribution using <strong>beta</strong> as the input. The Dirichlet distribution takes a number called <strong>beta</strong> for each <code>topic</code> (or category).</li><li>Each sample will fill out each column in the table, sum to one, and give the probability of each part per <code>topic</code>(column).</li></ul></li><li>Build the <code>**DOCUMENTS** VS **topics** table</code>.<ul><li>For each row, draw a sample from a Dirichlet distribution using <strong>alpha</strong> as the input. The Dirichlet distribution takes a number called alpha for each <code>topic</code> (or category).</li><li>Each sample will fill out each row in the table, sum to one, and give the probability of each <code>topic</code> (column) per DOCUMENT.</li></ul></li><li>Build the actual DOCUMENTS. For each DOCUMENT:<ul><li>Step_1) look up its <strong>row</strong> in the <code>**DOCUMENT** VS **topics** table</code>,</li><li>Step_2) sample a <code>topic</code> based on the probabilities in the row,</li><li>Step_3) go to the <code>**WORDS** VS **topics** table</code>,</li><li>Step_4) look up the <code>topic</code> sampled,</li><li>Step_5) sample a <strong>WORD</strong> based on the probabilities in the column,</li><li>Step_6) repeat from step 2 until you‚Äôve reached how many WORDS this DOCUMENT was set to have.</li></ul></li></ul></li></ul><hr><a href=#3-autoencoder><h2 id=3-autoencoder><span class=hanchor arialabel=Anchor># </span>3. AutoEncoder</h2></a><ul><li>As a <strong>Unsupervised method</strong>,</li></ul><a href=#variational-inference--neural-network--scalable-vi><h3 id=variational-inference--neural-network--scalable-vi><span class=hanchor arialabel=Anchor># </span>Variational Inference + Neural Network = Scalable VI</h3></a><p>10 years ago, people used to think that Bayesian methods are mostly suited for small datasets because it&rsquo;s computationally expensive. In the era of Big data, our Bayesian methods met deep learning, and people started to make some mixture models that has neural networks inside of a probabilistic model.</p><p>How to scale Bayesian methods to <code>large datasets</code>? The situation has changed with the development of <strong>stochastic Variational Inference</strong>, trying to solve the inference problem exactly without the help of sampling.</p><hr><a href=#-background-knowledge><h2 id=-background-knowledge><span class=hanchor arialabel=Anchor># </span>> Background Knowledge:</h2></a><p>Let&rsquo;s say we have a trouble with EM in GMM&mldr;saying that we cannot calculate the <code>MLE value</code> of the soft clusters???</p><p>This is the useful story when you cannot calculate the MLE value in the EM algorithm..
<img src=https://user-images.githubusercontent.com/31917400/86541806-ce988600-bf07-11ea-8dc6-9da63e6ee9f3.jpg>
When MLE does not work for the original margin of log-likelihood, then we try to get a <strong>lower bound</strong> with the function that we can easily optimize? Instead of maximizing the original margin of log-likelihood, we can maximize its <strong>lower bound</strong>!!</p><a href=#now-lets-find-the-lower-bound-to-estimate-the-mle-value><h2 id=now-lets-find-the-lower-bound-to-estimate-the-mle-value><span class=hanchor arialabel=Anchor># </span>Now, Let&rsquo;s find the Lower Bound to estimate the <code>MLE value</code>!</h2></a><blockquote><p>[note] But it&rsquo;s just a lower bound.. there is no guarantee that it gives us the correct parameter estimation!</p></blockquote><ul><li>Perhaps we can try&mldr;a <strong>family of lower bounds</strong>?? i.e. try <strong>many different lower bounds</strong>!</li><li><a href=#let-me-introduce-qt-as-the-variational-distribution-of-the-alpha-coefficient-mixing-coefficient-probability-of-the-hidden-membership-t-c-here-qt-are-not-fingers-any-more-like-in-the-typical-variational-inference--jansens-lower-boundsss-are-fingers-any-distribution-can-be-estimated-by-such-a-bunch-of-lower-bounds><h2 id=let-me-introduce-qt-as-the-variational-distribution-of-the-alpha-coefficient-mixing-coefficient-probability-of-the-hidden-membership-t-c-here-qt-are-not-fingers-any-more-like-in-the-typical-variational-inference--jansens-lower-boundsss-are-fingers-any-distribution-can-be-estimated-by-such-a-bunch-of-lower-bounds><span class=hanchor arialabel=Anchor># </span>Let me introduce <code>q(t)</code> as the variational distribution of the <code>alpha coefficient</code> (mixing coefficient: probability of the hidden membership <code>t</code>= c). Here, <code>q(t)</code> are not fingers any more like in the typical Variational Inference. Jansen&rsquo;s &ldquo;lower boundsss&rdquo; are fingers. Any distribution can be estimated by such a bunch of &ldquo;lower bounds&rdquo;!!!</h2></a></li><li>Develop a bunch of different <code>lower bounds</code>:<ul><li>Use (0)<code>Hidden "t" value</code>, and (1)<code>Alpha Coefficient: q(t)</code>, (2) <strong>log(</strong><code>p(x, t)/q(t)</code><strong>)</strong></li><li><strong>min{</strong> <code>q(t)</code>*log[<code>p(x,t)/q(t)</code>] <strong>}</strong> &mldr;KL-Divergence.. This is the Jensen&rsquo;s lower bound? Let&rsquo;s re-express our <code>log marginal</code> with KL-Divergence elements!</li><li>Imagine each finger(Jansen&rsquo;s lower bound) is a cluster ???
<img src=https://user-images.githubusercontent.com/31917400/86539994-1fed4900-bef9-11ea-8817-ed6243b4bcbb.jpg></li></ul></li></ul><p>General EM-Algorithm
<img src=https://user-images.githubusercontent.com/31917400/71264565-458b7a00-233c-11ea-88d6-e3316d5fef5b.jpg>
We built a lower bound on the local likelihood which depends both on the theta to maximize the local likelihood and the parameter q which is the variational distribution value, and it suggests we can optimize this lower bound in iterations by repeating the two steps until convergence. On the E-step, fix theta and maximize the lower bound with respect to q. And on the M-step, fix q and maximize the lower bound with respect of theta. So this is the general view of the expectation maximization.</p><a href=#is-it-just-coincidence-that-jansens-lower-bound-looks-like-kl-divergence><h2 id=is-it-just-coincidence-that-jansens-lower-bound-looks-like-kl-divergence><span class=hanchor arialabel=Anchor># </span>Is it just coincidence that <code>Jansen's lower bound</code> looks like <code>KL-Divergence</code>?</h2></a><a href=#now-we-just-found-the-first-jansens-bound-as-a-finger-how-many-more-fingers-to-go><h2 id=now-we-just-found-the-first-jansens-bound-as-a-finger-how-many-more-fingers-to-go><span class=hanchor arialabel=Anchor># </span>Now we just found the first Jansen&rsquo;s bound as a finger. How many more fingers to go?</h2></a><hr><a href=#variational-autoencoder-and-generative-model><h1 id=variational-autoencoder-and-generative-model><span class=hanchor arialabel=Anchor># </span>Variational Autoencoder and Generative model:</h1></a><p>How can we perform efficient inference and learning in directed probabilistic models, in the presence of <strong>continuous latent variables</strong> with <strong>intractable posterior distributions</strong>, and <strong>large datasets</strong>?</p><p>In contrast to the plain autoencoders, it has <code>sampling inside</code> and has <code>variational approximations</code>.</p><ul><li>for Dimensionality Reduction</li><li>for Information Retrieval</li></ul><blockquote><p>[INTRO]: Why fitting a certain distribution into the disgusting DATA (<strong>why do you want to model it</strong>)?</p></blockquote><ul><li>If you have super complicated objects like natural images, you may want to build a probability distribution such as &ldquo;GMM&rdquo; based on the dataset of your natural images then try to generate <strong>new complicated data</strong>&mldr;</li><li>Application?<ul><li><strong>Detect anomalies, sth suspicious</strong><ul><li>ex> For example, you have a bank and you have a sequence of transactions, and then, if you fit your probabilistic model into this sequence of transactions, for a new transaction you can predict how probable this transaction is according to our model, our current training data-set, and if this particular transaction is not very probable, then we may say that it&rsquo;s kind of suspicious and we may ask humans to check it.</li><li>ex> For example, if you have security camera footage, you can train the model on your normal day security camera, and then, if something suspicious happens then you can detect that by seeing that some images from your cameras have a low probability of your image according to your model.</li></ul></li><li><strong>Deal with N/A</strong><ul><li>ex> For example, you have some images with obscured parts, and you want to do predictions. In this case, if you have P(X) - probability distribution of your data -, it will help you greatly to deal with it.</li></ul></li><li><strong>Represent highly structured data in low dimensional embeddings</strong><ul><li>ex> For example, people sometimes build these kind of latent codes for molecules and then try to discover new drugs by exploring this space of molecules in this latent space&mldr;..??</li></ul></li></ul></li></ul><a href=#lets-model-the-image-px--yes-its-about-damn-large-sample-size-with-high-dimensionalityin-the-context-of-unsupervised-learning><h2 id=lets-model-the-image-px--yes-its-about-damn-large-sample-size-with-high-dimensionalityin-the-context-of-unsupervised-learning><span class=hanchor arialabel=Anchor># </span>Let&rsquo;s model the image <code>P(x)</code> ! Yes, it&rsquo;s about damn large sample size with high dimensionality..in the context of Unsupervised Learning.</h2></a><img src=https://user-images.githubusercontent.com/31917400/71101742-24495300-21af-11ea-9821-a14e07c54148.jpg><ul><li>[1.CNN]: Let&rsquo;s say that <strong>CNN</strong> will actually return your <strong>logarithm of probability</strong>.<ul><li>The problem with this approach is that you have to normalize your distribution. You have to make your distribution to sum up to one, with respect to sum according to all possible images in the world, and there are billions of them. So, this normalization constant is very expensive to compute, and you have to compute it to do the training or inference in the proper manner. HOW? You can use the chain rule. <code>Any probabilistic distribution can be decomposed into a product of some conditional distributions</code>, then we build these kind of conditional probability models to model our <code>overall joint probability</code>.</li></ul></li><li>[2.RNN]: how to represent these <code>conditional probabilities</code> is with <strong>RNN</strong> which basically will read your image pixel by pixel, and then outputs your prediction for the next pixel - Using proximity, Prediction for brightness for next pixel for example! And this approach makes modeling much easier because now normalization constant has to think only about 1D distribution.<ul><li>The problem with this approach is that you have to generate your new images one pixel at a time. So, if you want to generate a new image you have to first generate X1 from the marginal distribution X1, then you will feed this into the RNN, and it will output your distribution on the next pixel and etc. So, no matter how many computers you have, one high resolution image can take like minutes which is really long&mldr;</li></ul></li><li><a href=#3-our-pick-is-pdf-this-is-very-important-lets-find-the-density-model-of-our-data-predictive-distribution><h3 id=3-our-pick-is-pdf-this-is-very-important-lets-find-the-density-model-of-our-data-predictive-distribution><span class=hanchor arialabel=Anchor># </span>[3. Our pick is pdf] This is very important. Let&rsquo;s find the density model of our data (predictive distribution)!</h3></a><ul><li>We believe <code>x ~ Gaussian</code></li><li><strong>CNN with Infinite continuous GMM:</strong> In short, we can try <strong><code>infinite mixture of Gaussians</code> which can represent any probability distribution!</strong> Let&rsquo;s say if each object (image X) has a corresponding <strong>latent variable <code>t</code></strong>, and the image X is caused by this <strong><code>t</code></strong>, then we can marginalize out w.r.t <strong><code>t</code></strong>, and the conditional distribution <code>P(X|t)</code> is Gaussian. We can have a mixture of infinitely many Gaussians, for each value of <strong>&ldquo;t&rdquo;</strong>(membership?).<ul><li>Then we mix these Gaussian with <strong>weights</strong>(mixing coefficients). Yes. We are trying to use Neural Network (a.k.a weighting machine) inside this model at the end&mldr;</li><li>First, we should define the <strong>prior</strong> <code>P(t)</code> and the <strong>likelihood</strong> <code>P(x|t)</code> to model <code>P(x)</code> which is the Sum( <code>P(x,t)</code>: <strong>the un-normalized posterior</strong> )<ul><li>(1)<code>Prior</code> for the latent variable <strong>t</strong>: <code>P(t) = N(0, I)</code>.. oh yeah..the membership <code>t</code> around 0 &mldr; <strong>Done and Dusted</strong>.</li><li>(2)<code>Likelihood</code> for the data <strong>x</strong>: <code>P(x|t) = N( Œº(t), Œ£(t) )</code>&mldr;it can be a gaussian with parameters relying on <code>t</code>&mldr; <strong>This is tricky</strong>!<ul><li><code>Œº(t)</code> = W*<code>t</code> + b (Of course, each component&rsquo;s location would be subject to the membership <code>t</code>)</li><li><code>Œ£(t)</code> =
<img src="https://render.githubusercontent.com/render/math?math=%5cSigma_0" width=auto alt=formula> (Of course, each component&rsquo;s size would be subject to the membership <code>t</code>)</li><li>REALLY???? Here we are skeptical about the above linearity of the parameterization..<ul><li><code>Œº(t)</code> =
<img src="https://render.githubusercontent.com/render/math?math=CNN_1%28t%29" width=auto alt=formula>..if you input <code>t</code>, this CNN outputs the mean? blurry? <code>image vector</code>!</li><li><code>Œ£(t)</code> =
<img src="https://render.githubusercontent.com/render/math?math=CNN_2%28t%29" width=auto alt=formula>..if you input <code>t</code>, this CNN outputs the <code>Cov matirx</code></li><li><code>CNN</code> generate weights <code>w</code>&mldr;at the end..CNN is just giving you a bunch of weights to your likelihood..like a weighting machine. Let&rsquo;s say the <code>w</code> is another parameter&mldr;it&rsquo;s like&mldr;<strong>mixing coefficient</strong>?<ul><li><img src="https://render.githubusercontent.com/render/math?math=CNN_1%28t%29" width=auto alt=formula> -> <code>Œº(t|w)</code></li><li><img src="https://render.githubusercontent.com/render/math?math=CNN_2%28t%29" width=auto alt=formula> -> <code>Œ£(t|w)</code>&mldr; problem is that this is too huge&mldr;<ul><li>How about Let CNN ignores other covariance values except &ldquo;diag(
<img src="https://render.githubusercontent.com/render/math?math=%5csigma%5e2%28t,w%29" width=auto alt=formula>)&rdquo;<ul><li><code>Œ£(t|w)</code> -> &ldquo;diag(
<img src="https://render.githubusercontent.com/render/math?math=%5csigma%5e2%28t,w%29" width=auto alt=formula>)&rdquo;</li></ul></li></ul></li></ul></li></ul></li></ul></li></ul></li><li>Now, let&rsquo;s train our model! Find the partameters - <code>t</code>, <code>w</code><ul><li><code>MLE</code>: Can you get some probability values for each datapoint? Let&rsquo;s maximize the density of our data given the parameters - <code>w</code>,<code>t</code> ? What is important is that the mixing coefficient <code>w</code> depends on <code>t</code>. If we have a latent variable, it&rsquo;s natural to go with Generalized EM-Algorithm, building <code>Jansen's bounds</code> on the MLE and maximize the sum of those bounds! But&mldr;you cannot imagine the analytical form of the likelihood <code>P(x|t)</code> = N(
<img src="https://render.githubusercontent.com/render/math?math=CNN_1%28t,w%29" width=auto alt=formula>,
<img src="https://render.githubusercontent.com/render/math?math=CNN_2%28t,w%29" width=auto alt=formula> ). So..we can&rsquo;t get Sum of joints ???<ul><li>SUM(<strong><code>log[P(x|w)]</code></strong> per each observation<code>x</code>)..so try to come up with another &ldquo;SUM&rdquo; caused by the latent variable <code>t</code>.</li></ul></li><li><code>MCMC</code>? <code>VI</code>? &mldr; ok, then can we obtain the un-normalized posterior:<code>P(x,t)</code>? Although knowing the prior <code>P(t)</code>, you cannot imagine the analytical form of the likelihood <code>P(x|t)</code> = N(
<img src="https://render.githubusercontent.com/render/math?math=CNN_1%28t,w%29" width=auto alt=formula>,
<img src="https://render.githubusercontent.com/render/math?math=CNN_2%28t,w%29" width=auto alt=formula> ). So..we can&rsquo;t get Sum of joints???</li><li>Anyway, we decide our predictive null model - mean(x) - is explained by Neural Network&mldr;.</li><li>Then how to train? How to find the parameter - <code>t</code>,<code>w</code> - in the first place?</li></ul></li></ul></li></ul></li></ul><a href=#you-know-what-we-are-start-off-with-decoder-><h2 id=you-know-what-we-are-start-off-with-decoder-><span class=hanchor arialabel=Anchor># </span>You know what? we are start off with Decoder ?</h2></a><p>Only if we have <code>hidden variables</code>&mldr;
<img src=https://user-images.githubusercontent.com/31917400/72342676-f3344380-36c4-11ea-90a2-ea05caf2e11a.jpg></p><ul><li><a href=#overview-get-some-variational-distribution-qt-or-lbt-><h2 id=overview-get-some-variational-distribution-qt-or-lbt-><span class=hanchor arialabel=Anchor># </span>Overview: get some variational distribution <code>q(t)</code> or <code>LB(t)</code> ?</h2></a></li></ul><img src=https://user-images.githubusercontent.com/31917400/86674224-5bb70a00-bff0-11ea-91d6-c6907d62ae6a.jpeg><ul><li><p>*In VI, the <strong>KL-Divergence</strong> (where each <code>q(z)</code> is a <code>finger</code>) should be <strong><code>minimized</code></strong>. In VI, the <strong>MLE estimator</strong> is the joint of all <code>q(z)</code>,</p></li><li><p>*In VAE, the <code>Jansen's lower bound</code> as a <strong>KL-Divergence</strong> needs to be <strong><code>maximized</code></strong>..(where <code>q(z)</code> is a mixing coefficient for GMM form with <code>log(p/q)</code> as a Gaussian cluster) and each Jansen&rsquo;s lower bound is a <code>finger</code>. In VAE, the <strong>MLE estimator</strong> is the sum of a bunch of <code>Jansen's lower bound</code>ssss.</p></li><li><p>*In VAE, the latent space will have highly compressed patterns that were learned from the data. Anomalies will not &ldquo;fit&rdquo; into the scheme of the latent vector and the abnormal part will get lost when generating the output.</p><ul><li>That means not only will the Input-Output difference in VAE be larger than with that in AE, you will also be able to locate the abnormal part in a single sample.</li><li>As for Anomaly detection task, you can compare the data with the latent spaces not the outputs, thus we won‚Äôt need the reconstruction part of the network?</li></ul></li><li><p><strong>[Story]:</strong> <code>**Encoding**: Discover the latent parameter from our dataset</code> -> <code>**Decoding**: Generate new data based on the latent memberships</code></p><ul><li>Ok, let&rsquo;s do some reverse enigineering. <strong>Back to the question. How to train?</strong> How to find the parameter in the first place?</li><li><a href=#t-and-w-is-the-key><h2 id=t-and-w-is-the-key><span class=hanchor arialabel=Anchor># </span><code>t</code> and <code>w</code> is the key!</h2></a><ul><li>Let&rsquo;s try <strong><code>Variational Inference</code></strong>. Assuming each <strong>q(
<img src="https://render.githubusercontent.com/render/math?math=t_i" width=auto alt=formula>)</strong> as the Exponential family function = N(
<img src="https://render.githubusercontent.com/render/math?math=m_i" width=auto alt=formula>,
<img src="https://render.githubusercontent.com/render/math?math=s_i%5e2" width=auto alt=formula>), so each <code>q(t)</code> is different Gaussian&mldr;and the value is probability as a mixing coefficient. Then we can <strong>maximize the Jansen&rsquo;s Lower Bound</strong> w.r.t <code>q</code>, <code>m</code>, <code>s^2</code>. But it&rsquo;s so complicated..Is there other way? -> VAE&mldr;</li><li>Both <code>t</code>,<code>w</code> are parameters for Decoding&mldr;our final predictive model.</li><li>The solution is &ldquo;Encoding&rdquo; since it returns the distribution of <code>t</code>. Remember? <code>w</code>(NN weighting) relies on <code>t</code>(membership).</li><li>Hey, so we first want to obtain the latent variable space! We are conjuring the <strong>Encoder</strong> that outputs the latent parameter <code>t</code> space since <code>w</code> results from <code>t</code>. Let&rsquo;s find the posterior <code>P(t|x)</code> from an Encoder..then we would someday get our final <code>P(x|t)</code> from a Decoder.<ul><li><p><strong>[Find <code>t</code>]</strong> <strong>Bring up the &ldquo;factorized&rdquo; variational distribution <code>q(t)</code></strong> and let NN return (
<img src="https://render.githubusercontent.com/render/math?math=m_i" width=auto alt=formula>,
<img src="https://render.githubusercontent.com/render/math?math=s_i%5e2" width=auto alt=formula>) that explains the distribution of <code>t</code> <strong>which is a Gaussian</strong> and we call it <code>q(t)</code> function &mldr; we can say that the latent variable <code>t</code> follows Gaussian.<br>-: Let&rsquo;s make <code>q(t)</code>= N(m, s^2) flexible. If assume all <strong>q(
<img src="https://render.githubusercontent.com/render/math?math=t_i" width=auto alt=formula>)</strong> = N(
<img src="https://render.githubusercontent.com/render/math?math=CNN_1%28t%29" width=auto alt=formula>=<code>m(x_i, œÜ)</code>,
<img src="https://render.githubusercontent.com/render/math?math=CNN_2%28t%29" width=auto alt=formula>=<code>s^2(x_i, œÜ)</code> ), then the training get much easier. Since we already have the original input data <code>x</code>, we can simply ask CNN to produce weight <code>œÜ</code>.
<img src=https://user-images.githubusercontent.com/31917400/86669433-7dfa5900-bfeb-11ea-9160-c33cde0b9c08.jpg></p><p>-: Once we pass our initial data <code>x</code> through our [first neural network] as an encoder with parameters<code>œÜ</code>, it returns <code>m</code>,<code>s^2</code> which are parameters of <code>q(t)</code>. How <code>t</code> are distributed? Normally&mldr;</p><ul><li><a href=#we-found-qt--ptx--nm-s2-which-is-an-unique-mixing-coefficient-functioninterestingly-we-forget-about-the-mixing-coefficient-and-simply-do-montecarlo-sampling-from-this-distributionqt-to-get-random-data-pt-t><h2 id=we-found-qt--ptx--nm-s2-which-is-an-unique-mixing-coefficient-functioninterestingly-we-forget-about-the-mixing-coefficient-and-simply-do-montecarlo-sampling-from-this-distributionqt-to-get-random-data-pt-t><span class=hanchor arialabel=Anchor># </span>We found <code>q(t)</code> = <code>P(t|x)</code> = <code>N(m, s^2)</code> which is an unique mixing coefficient function&mldr;Interestingly, we forget about the mixing coefficient and simply do MonteCarlo Sampling from this distribution<code>q(t)</code> to get random data pt <code>t</code>.</h2></a></li></ul></li><li><p><strong>[Find <code>w</code>]</strong> Now, we know <code>t</code> so we can get <code>w</code>! Let&rsquo;s pass this sampled vector <code>T</code> into the <code>second neural network</code> to get parameters<code>w</code>.
-: It outputs us the distribution that are as close to the input data as possible.
<img src=https://user-images.githubusercontent.com/31917400/86837661-1d285a80-c097-11ea-936f-8dbafdce6945.jpg></p></li></ul></li></ul></li></ul></li></ul><p>[note] <strong><code>Latent variable distribution: q(t)</code> is useful!</strong> Anomaly Detection for a new image which the network never saw, of some suspicious behavior or something else, our conditional neural network of the encoder can output your <strong>latent variable distribution</strong> as far away from the Gaussian. By looking at the distance between the variational distribution <code>q(t)</code> and the standard Gaussian, you can understand how anomalistic a certain point is &mldr; wait. <code>P(t)</code> is Standard Normal?<br><img src=https://user-images.githubusercontent.com/31917400/72226852-bca7dd00-358d-11ea-98d6-20965d0dce46.jpg></p><a href=#next-how-to-define-the-cnns-weighting-mechanism-for-œÜ-and-w---keep-maximizing-jensens-lower-bound-via-each-gradient-calculation-in-cnn><h2 id=next-how-to-define-the-cnns-weighting-mechanism-for-œÜ-and-w---keep-maximizing-jensens-lower-bound-via-each-gradient-calculation-in-cnn><span class=hanchor arialabel=Anchor># </span>Next, how to define the CNN&rsquo;s weighting mechanism for <code>Œ¶</code> and <code>w</code> ? : Keep maximizing <code>Jensen's Lower bound</code> via each gradient calculation in CNN!</h2></a><ul><li>Jensen&rsquo;s LB is the <strong>objective function</strong> in VAE (like.. MSE lost function in LM)</li><li><strong>Gradient of Encoder:</strong> Make an Expected Value ?<ul><li>we&rsquo;re passing our image through our Encoder, and compute the <strong>usual gradient</strong> of this first neural network with respect to its parameters <code>Œ¶</code> to get the parameters(Œ¶) of the variation distribution <code>q(t|Œ¶)</code>. We use <strong>&ldquo;log derivitive trick&rdquo;</strong> to approximate the gradient (make the form of expected value?) but it has some problem: <code>the variance of this stochastic approximation will be so high that you will have to use lots and lots of gradients to approximate this thing accurately</code>. How can we estimate this gradient with a much <strong>smaller variance estimate</strong>?</li></ul></li><li><strong>Gradient of Decoder:</strong> Make an Expected Value ?<ul><li><p>we sample <code>t</code> from the variation distribution <code>q(t|Œ¶)</code> and put this <code>point</code> as input to the Decoder with parameters <code>w</code>. And then we just compute the <strong>usual gradient</strong> of this second neural network with respect to its parameters <code>w</code>.<br><img src=https://user-images.githubusercontent.com/31917400/72433990-7a9bb880-3792-11ea-8cfd-f3e6778fa8ad.jpg></p></li><li><p><strong>Issues of gradient of Encoder:</strong> ÌóàÎ≤åÏ∞Ω Í∑∏ÎùºÎîîÏñ∏Ìä∏Ïó¨? How can we better estimate this varying gradient with a much <strong>smaller variance estimate</strong>?</p><ul><li>Ïôú ÌóàÎ≤åÏ∞Ω? our input data (x) is Ïù¥ÎØ∏ÏßÄÎãàÍπê&mldr;</li><li>when sampling <code>t</code>, <strong>&ldquo;reparameterization trick&rdquo;</strong> of our latent variable makes the a Jensen&rsquo;s lower bound estimator easy to be optimized using standard stochastic gradient.</li><li>so..you just sample from a identity matrix&mldr;All works will be done by <code>m</code> and <code>s^2</code>..
<img src=https://user-images.githubusercontent.com/31917400/73176973-afe6c580-4105-11ea-8822-49b2d202c156.jpg></li></ul></li></ul></li></ul><hr><a href=#general-learning-with-latent-priors><h3 id=general-learning-with-latent-priors><span class=hanchor arialabel=Anchor># </span>General Learning with latent priors</h3></a><p>We first pick a fake? posterior <code>q(z|v)</code> as a <strong>family of distributions</strong> over the <code>latent variables</code> with <strong>its own variational parameters</strong><code>v</code>. KL-divergence method helps us to minimize the distance between <code>P(z)</code> and <code>q(z)</code>, and in its optimization process, we can use <code>mini-batching</code> training strategy(since its likelihood can be split into many pieces of log sum), which means we don&rsquo;t need to compute the whole training of the likelihood. <code>ELBO supports mini-batching</code>.</p><ul><li>We can use MonteCarlo estimates for computing stochastic gradient, which is especially useful when the reparameterization trick for <code>q(z|v)</code> is applicable.
<img src=https://user-images.githubusercontent.com/31917400/69436481-5b0b8500-0d39-11ea-8e3d-1d565674042e.jpg></li></ul><a href=#variational-dropout-and-scalable-bnn><h3 id=variational-dropout-and-scalable-bnn><span class=hanchor arialabel=Anchor># </span>Variational Dropout and Scalable BNN</h3></a><p>Compress NN, then fight severe overfitting on some complicated datasets.</p></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://quartz.jzhao.xyz/js/graph.afdb02e537635f9a611b53a988e5645b.js></script></div></div><div id=contact_buttons><footer><p>Made by Jacky Zhao using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, ¬© 2022</p><ul><li><a href=https://quartz.jzhao.xyz/>Home</a></li><li><a href=https://twitter.com/_jzhao>Twitter</a></li><li><a href=https://github.com/jackyzha0>Github</a></li></ul></footer></div></div></body></html>