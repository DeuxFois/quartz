<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>MLIA on</title><link>https://quartz.jzhao.xyz/mlia/</link><description>Recent content in MLIA on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://quartz.jzhao.xyz/mlia/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://quartz.jzhao.xyz/MLIA/Classification/Bayes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.jzhao.xyz/MLIA/Classification/Bayes/</guid><description>Bayes Law $$ \underbrace{\mathbb{P}(Y \mid X)}_{\text {Posterior probability }}=\frac{\overbrace{\mathbb{P}(Y)}^{\text {Prior probability}} \cdot \overbrace{\mathbb{P}(X \mid Y)}^{\text {Likelihood}}}{\mathbb{P}(X)} $$ the denominator is given by $\mathbb{P}(X) = {\mathbb{P}(X|Y=\mathbf{1})}\mathbb{P}(Y=1)+{\mathbb{P}(X|Y=\mathbf{0})}\mathbb{P}(Y=0)$</description></item><item><title/><link>https://quartz.jzhao.xyz/MLIA/Classification/Evaluation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.jzhao.xyz/MLIA/Classification/Evaluation/</guid><description/></item><item><title/><link>https://quartz.jzhao.xyz/MLIA/Classification/KNN/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.jzhao.xyz/MLIA/Classification/KNN/</guid><description/></item><item><title/><link>https://quartz.jzhao.xyz/MLIA/Clustering/Introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.jzhao.xyz/MLIA/Clustering/Introduction/</guid><description>Organiser les par groupes. $\left{\mathbf{x}{i}\right}{i=1}^{n} \rightarrow\left{\hat{y}{i}\right}{i=1}^{n}$ $\hat{y} \in \mathcal{Y}$ représente un cluster
![[Pasted image 20220406111839.png]]
k-means ( $k$-moyennes). Mélange de gaussiennes Clustering hiérarchique</description></item><item><title/><link>https://quartz.jzhao.xyz/MLIA/Clustering/Kmeans/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.jzhao.xyz/MLIA/Clustering/Kmeans/</guid><description>Kmeans centroïde $\boldsymbol{\mu}=\frac{1}{m} \sum_{i}^{m} \mathbf{x}_{i}$ inertie $\mathcal{I}{T}=\sum{\mathbf{x}{i}}\left|\mathbf{x}{i}-\boldsymbol{\mu}\right|^{2}$ ![[Pasted image 20220406110758.png]]
Inertie L&amp;rsquo;inertie $\mathcal{I}{T}$ d&amp;rsquo;un nuage des points est représentée par la distance au carré des points à leur centroïde $$\sum{\mathbf{x}{i}}\left|\mathbf{x}{i}-\boldsymbol{\mu}\right|^{2}=\sum_{c=1} m_{c}\left|\boldsymbol{\mu}{c}-\boldsymbol{\mu}\right|^{2} \quad+\sum{c=1} \sum_{\mathbf{x}{i} \mid \hat{y}{i}=c}\left|\mathbf{x}{i}-\boldsymbol{\mu}{c}\right|^{2} $$</description></item><item><title/><link>https://quartz.jzhao.xyz/MLIA/densit%C3%A9_proba/Introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.jzhao.xyz/MLIA/densit%C3%A9_proba/Introduction/</guid><description>densité proba Estimer la loi de proba des données
Modèle peut être génératif $\left{\mathbf{x}{i}\right}{i=1}^{n} \Rightarrow p(\mathbf{x})$ $p(\mathbf{x})$ est une densité de proba $\left(\int p(\mathbf{x}) d \mathbf{x}=1\right)$ Paramètres : Type de loi (gaussienne, &amp;hellip;) Paramètres de la loi $(\mu, \Sigma)$ !</description></item><item><title/><link>https://quartz.jzhao.xyz/MLIA/Generative-vs-Discriminative++/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.jzhao.xyz/MLIA/Generative-vs-Discriminative++/</guid><description>[Note] Variations on supervised and unsupervised Model ? Semi-Supervised Learning: we have a bunch of pairs (x1,y1), (x2,y2), &amp;hellip;(x_i,y_i), and then we are additionally given more x values such as x_i+1, x_i+2,.</description></item><item><title/><link>https://quartz.jzhao.xyz/MLIA/Generative-vs-Discriminative/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.jzhao.xyz/MLIA/Generative-vs-Discriminative/</guid><description>Generative v.s. Discriminative Models Intro
What&amp;rsquo;s Generative or Discriminative model? Let&amp;rsquo;s say you have input data x and you want to classify the data into labels y.</description></item><item><title/><link>https://quartz.jzhao.xyz/MLIA/Intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.jzhao.xyz/MLIA/Intro/</guid><description>Generative vs Discriminative
Apprentissage non-supervisé $\mathbf{x} \in \mathbb{R}^{d}$ est une observation de $d$ variables réelles. L&amp;rsquo;ensemble d&amp;rsquo;apprentissage est définit par les observations $\lbrace{x}_{i}\rbrace_{i=1}^{m}$ où $n$ est le nombre d&amp;rsquo;exemples d&amp;rsquo;apprentissages (de points).</description></item><item><title/><link>https://quartz.jzhao.xyz/MLIA/Regression/Batch-vs-stohastic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.jzhao.xyz/MLIA/Regression/Batch-vs-stohastic/</guid><description>Batch vs Stochastic Batch Gradient Descent BGD is a variation of the gradient descent algorithm that calculates the error for each eg in the training datasets, but only updates the model after all training examples have been evaluated.</description></item><item><title/><link>https://quartz.jzhao.xyz/MLIA/Regression/Linear-Regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.jzhao.xyz/MLIA/Regression/Linear-Regression/</guid><description>Regression linéaire Données d&amp;rsquo;apprentissage : $\lbrace\mathbf{x}_{i}, y_{i} \rbrace_{i=1}^{m}$
observations (entrées) $: \mathbf{x}_{i} \in \mathbb{R}^{d}$
mesure d&amp;rsquo;intérêt (sortie à prédire) : $y_{i} \in \mathcal{Y}$</description></item><item><title/><link>https://quartz.jzhao.xyz/MLIA/Regression/Logistic-Regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.jzhao.xyz/MLIA/Regression/Logistic-Regression/</guid><description/></item><item><title/><link>https://quartz.jzhao.xyz/MLIA/Regression/Perceptron/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.jzhao.xyz/MLIA/Regression/Perceptron/</guid><description/></item><item><title/><link>https://quartz.jzhao.xyz/MLIA/R%C3%A9duction_dimension/ACP/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.jzhao.xyz/MLIA/R%C3%A9duction_dimension/ACP/</guid><description>Objectif Condenser l’information de la matrice des donn´ees afin d’en retirer les relations caractéristiques (ressemblances entre observations et liaisons entre variables) tout en limitant la perte d’information.</description></item><item><title/><link>https://quartz.jzhao.xyz/MLIA/R%C3%A9duction_dimension/Covariances-et-corr%C3%A9lations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.jzhao.xyz/MLIA/R%C3%A9duction_dimension/Covariances-et-corr%C3%A9lations/</guid><description>Covariance et Corrélations Matrice des variances-covariances $$ \begin{aligned} S=\operatorname{Var}(\mathbf{X}) &amp;amp;=\left[\begin{array}{ccccc} \operatorname{Var}\left(X_{1}\right) &amp;amp; \operatorname{Cov}\left(X_{1}, X_{2}\right) &amp;amp; \cdots &amp;amp; \operatorname{Cov}\left(X_{1}, X_{d}\right) \ \operatorname{Cov}\left(X_{2}, X_{1}\right) &amp;amp; \ddots &amp;amp; \cdots &amp;amp; \operatorname{Cov}\left(X_{2}, X_{d}\right) \ \vdots &amp;amp; &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \ \operatorname{Cov}\left(X_{d}, X_{1}\right) &amp;amp; \operatorname{Cov}\left(X_{d}, X_{2}\right) &amp;amp; \cdots &amp;amp; \operatorname{Var}\left(X_{d}\right) \end{array}\right] \ &amp;amp;=\left[\begin{array}{cccc} s_{X_{1}}^{2} &amp;amp; s_{X_{1}, X_{2}}^{2} &amp;amp; \cdots &amp;amp; s_{X_{1}, X_{d}}^{2} \ s_{X_{2}, X_{1}}^{2} &amp;amp; \ddots &amp;amp; \cdots &amp;amp; s_{X_{2}, X_{d}}^{2} \ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \ s_{X_{d}, X_{1}}^{2} &amp;amp; s_{X_{d}, X_{2}}^{2} &amp;amp; \cdots &amp;amp; s_{X_{d}}^{2} \end{array}\right] \end{aligned} $$ avec $s_{j, j^{\prime}}^{2}$ la covariance entre les variables $X_{j}$ et $X_{j^{\prime}}$, tel que $$ s_{X_{j}, X_{j^{\prime}}}^{2}=\sum_{i=1}^{m} p_{i}\left(x_{i j}-\mu_{j}\right)\left(x_{i j^{\prime}}-\mu_{j^{\prime}}\right) $$ La covariance mesure la liaison linéaire qui peut exister entre un couple de variables quantitatives.</description></item><item><title/><link>https://quartz.jzhao.xyz/MLIA/R%C3%A9duction_dimension/Introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.jzhao.xyz/MLIA/R%C3%A9duction_dimension/Introduction/</guid><description>Comment décrire les données ? Approche 1 effectuer une analyse descriptive multidimensionelle ⊖ trop longue et souvent trop complexe
Approche 2 : utiliser des méthodes d’analyse des données ex: les méthodes factorielles comme l’Analyse en Composantes Principales (ACP) • Synthèse : réduire la dimension du problème tout en restituant le maximum d’information • Descriptif et exploratoire : visualisation des données (production de graphiques simples)</description></item><item><title/><link>https://quartz.jzhao.xyz/MLIA/Stats/Gaussian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.jzhao.xyz/MLIA/Stats/Gaussian/</guid><description>Loi normale En théorie des probabilités et en statistique, les lois normales sont parmi les lois de probabilité les plus utilisées pour modéliser des phénomènes naturels issus de plusieurs événements aléatoires.</description></item></channel></rss>